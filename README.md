
### 注：训练函数fit(x,y)中x需要是二维的，y是一维的
        特征工程:（数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已）其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。特征处理是特征工程的核心，包括数据预处理，特征选择，降维等。


# K-means
K-means聚类算法  
k-means算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似
度，而簇间的相似度较低。
其处理过程如下：   
1.随机选择k个点作为初始的聚类中心；   
2.对于剩下的点，根据其与聚类中心的距离，将其归入最近的簇   
3.对每个簇，计算所有点的均值作为新的聚类中心   
4.重复2、3直到聚类中心不再发生改变   

K-means.py主要是对31个省份的消费情况进行聚类，共分为6个簇，打印出每个簇的包含的城市及平均消费水平。

# DBSCAN
### DBSCAN算法是一种基于密度的聚类算法：
• 聚类的时候不需要预先指定簇的个数   
• 最终的簇的个数不定   
### DBSCAN算法将数据点分为三类：   
• 核心点：在半径Eps内含有超过MinPts数目的点   
• 边界点：在半径Eps内点的数量小于MinPts，但是落在核心点的邻域内    
• 噪音点：既不是核心点也不是边界点的点   

DBSCAN.py主要是根据学生的上网时长进行聚类分析。  
 
# PCA
### 主成分分析（PCA）可以把具有相关的高维变量合成为线性无关的低维变量，称为主成分。主成分能够尽可能保存原始数据的信息。
### 在介绍 PCA 的原理之前需要回顾涉及到相关术语：
• 方差    
• 协方差    
• 协方差矩阵    
• 特征向量和值    

方差：是各个样本和均值的平方和的均值，用来度量一组数据的分散程度。  
协方差：用于度量两个变之间的线性相关程度，若协方差为 0，则可认为二者线性无关。协方差矩阵是由变量的值 构成的矩阵（对称阵）
特征向量：矩阵的特征向量是描述数据集结构的非零向量。

主成分分析原理：
矩阵的主成分就是其协方差对应的特征向量，按照对应的特征值大小进行排序，最后的特征值就是第一主成分，其次是第二主成分，以此类推。

PCA.py主要是鸢尾花数据进行降维，并以散点图展现降维后鸢尾花类别。  
 
# NMF
非负矩阵分解（NMF）是在矩阵中所有元素均为非负数约束条件之下的矩阵分解方法。    
### 基本思想  
给定一个非负矩阵v，NMF能够找到一个非负矩阵w和一个非负矩阵h，使得矩阵w和h的乘积近似等于矩阵v中的值。  

NMF.py主要是用来人脸数据特征提取，同时程序进行了PCA和NMF提取的特征进行对比。  

# 图像分割
### 图像分割常用方法：
1.阈值分割：对图像灰度值进行度量，设置不同类别的阈值，达到分割的目的。   
2.边缘分割：对图像边缘进行检测，即检测图像中灰度值发生跳变的地方，则为一片区域的边缘。   
3.直方图法：对图像的颜色建立直方图，而直方图的波峰波谷能够表示一块屈原的眼神值的范围，来达到分割的目的。  
4.特定理论：基于聚类分析、小波变换等理论完成图像分割。  

K-means2Image.py主要目的是利用K-means聚类算法对图像像素点颜色进行聚类实现简单的图像分割。  

# 监督学习中的评价指标   
1.准确率（accuracy）  
2.精确率（precision）  
召回率（recall）   
例子：假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本，系统查找出50个，其中只有40个是真正的正样本，计算上述各指标。  
TP: 将正类预测为正类数  40   
FN: 将正类预测为负类数  20   
FP: 将负类预测为正类数  10   
TN: 将负类预测为负类数  30   
准确率(accuracy) = 预测对的/所有 = (TP+TN)/(TP+FN+FP+TN) = 70%   
精确率(precision) = TP/(TP+FP) = 80%   
召回率(recall) = TP/(TP+FN) = 2/3   
结论：其实就是分母不同，一个分母是预测为正的样本数（精确率），一个是原来样本中所有的正样本数（召回率）。  

# KNN（K近邻分类器）
KNN：通过计算待分类数据点，与已有数据集中的所有数据点的距离。取距离最小的前K个点，根据“少数服从多数”的原则，将这个数据点划分为出现次数最多的那个类别。 

# 决策数
决策数是一种树形结构的分类器，通过顺序询问分类点的属性决定分类点最终的类别。通常根据特征的信息增益或其他指标，构建一颗决策数。在分类时，只需要按照决策数中的结点依次进行判断，即可得到样本所属类别。

# 朴素贝叶斯
朴素贝叶斯分类器是一个以贝叶斯定理为基础的多分类的分类器。对于给定数据，首先基于特征的条件独立性假设，学习输入输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。  

KNN_DT_nbGB.py主要是分别训练KNN,DT和GNB模型，最后通过精确度，召回率和F1值指标进行评估。  

# 支持向量机(SVM)
SVM是一个分类算法，通过找到一个分类平面，将数据分隔在平面两侧，从而达到分类的目的。  
参考: https://www.zhihu.com/question/21094489

# 线性回归
1.线性回归(Linear Regression)是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。
2.线性回归利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。

Linear_model.py主要对房屋成交信息建立回归方程，并依据回归方程对房屋价格进行预测。   
Polynomial_model.py主要是改进Linear_model.py,使用多项式回归方程对房屋价格进行预测。  

# 岭回归
岭回归(Ridge Regression)是一种专用于共线性数据分析的有偏估计回归方法，是一种改良的最小二乘估计法，对某些数据的拟合要强于最小二乘法。

Ridge.py主要是根据已有的数据创建多项式特征，使用岭回归模型代替一般线性模型，对车流量的信息进行多项式回归  